{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entregar Modelo como Gradio App (Módulo 8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construindo um App no Gradio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta aula, vamos aprender uma nova forma de entregar um modelo para o usuário interagir. Em vez de criar uma API, vamos criar uma interface visual simples usando o Gradio. Vamos importar os módulos necessários, como o Gradio, o JobLib e o Pandas. Em seguida, vamos carregar o modelo salvo anteriormente usando o JobLib. Depois, vamos criar uma função chamada Predictive, que receberá os parâmetros de entrada da interface visual e fará a predição com base nesses dados. Em seguida, vamos criar a interface visual usando o Gradio, definindo os inputs e o output desejados. No próximo vídeo, veremos como chamar essa interface para interagir com o usuário.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import joblib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregar modelo (arquivo .plk)\n",
    "lr_multiple_model = joblib.load(\"../data/model_lr_multiple.plk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definir função de predição\n",
    "def predict(grupo_sanguineo, fumante, nivel_atividade_fisica, idade, peso, altura):\n",
    "    _fumante = \"Sim\" if fumante else \"Não\"\n",
    "    prediction_values = {\n",
    "        \"grupo_sanguineo\": grupo_sanguineo,\n",
    "        \"fumante\": _fumante,\n",
    "        \"nivel_atividade_fisica\": nivel_atividade_fisica,\n",
    "        \"idade\": idade,\n",
    "        \"peso\": peso,\n",
    "        \"altura\": altura,\n",
    "    }\n",
    "\n",
    "    predict_df = pd.DataFrame(prediction_values, index=[1])\n",
    "    colesterol = lr_multiple_model.predict(predict_df)\n",
    "\n",
    "    return colesterol.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definir interface gradio\n",
    "demo = gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs=[\n",
    "        gr.Radio([\"O\", \"A\", \"B\", \"AB\"]),\n",
    "        \"checkbox\",\n",
    "        gr.Radio([\"Baixo\", \"Moderado\", \"Alto\"]),\n",
    "        gr.Slider(20, 80, step=1),\n",
    "        gr.Slider(40, 160, step=0.1),\n",
    "        gr.Slider(150, 200, step=1),\n",
    "    ],\n",
    "    outputs=[\"number\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testando nosso App no Gradio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta aula, exploramos como fazer a conversão de uma variável booleana para uma variável de texto antes de enviá-la para o modelo. Foi explicado que o modelo espera receber \"sim\" ou \"não\" em vez de \"true\" ou \"false\". Para fazer essa conversão, criamos uma nova variável que assume o valor \"sim\" se a variável booleana for verdadeira e \"não\" caso contrário. Em seguida, executamos novamente a demonstração com a nova variável convertida. Também aprendemos como interagir com o modelo usando uma interface visual, que pode ser acessada localmente ou por meio de um link gerado pelo Gradle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rodar interface do modelo de predição no localhost\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7864\n"
     ]
    }
   ],
   "source": [
    "# finalizar interface - fechar a porta do localhost\n",
    "# demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "supervised_algorithms-WLhrZFFA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
